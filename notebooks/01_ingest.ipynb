{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840abb5d-6071-4805-92df-c91345576a7b",
   "metadata": {},
   "source": [
    "## 1. Ingesta (Capa Bronce)\n",
    "# Convertimos CSV crudo a formato Delta Lake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a21e6160-7b0d-4a83-a363-df067cc320ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from delta import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21928a5-ddb5-4cee-aed2-d98b5569cd8c",
   "metadata": {},
   "source": [
    "# URL del Master (definida en docker-compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c60ab7-7304-4155-b4f4-d3388d6407af",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_url = \"spark://spark-master:7077\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f60a15-de7d-4904-a38b-c8d630c0868c",
   "metadata": {},
   "source": [
    "# Configuración: Añadimos Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a7b1339-38d6-4a62-99f3-df1c0570749b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-952a6e2d-3132-4be4-8d39-8a51224b6b99;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 286ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-952a6e2d-3132-4be4-8d39-8a51224b6b99\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n",
      "26/02/02 01:43:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Lab_SECOP_Bronze\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.executor.memory\", \"1g\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "379b3a72-846d-4de8-ab94-9fd44af52f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo CSV crudo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/02 01:44:18 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "26/02/02 01:44:26 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/02/02 01:44:41 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas originales:\n",
      "['Entidad', 'Nit Entidad', 'Departamento', 'Ciudad', 'Estado', 'Descripcion del Proceso', 'Tipo de Contrato', 'Modalidad de Contratacion', 'Justificacion Modalidad de Contratacion', 'Fecha de Firma', 'Fecha de Inicio del Contrato', 'Fecha de Fin del Contrato', 'Precio Base', 'Valor Total', 'Valor Pagado']\n",
      "\n",
      "Columnas limpias:\n",
      "['entidad', 'nit_entidad', 'departamento', 'ciudad', 'estado', 'descripcion_del_proceso', 'tipo_de_contrato', 'modalidad_de_contratacion', 'justificacion_modalidad_de_contratacion', 'fecha_de_firma', 'fecha_de_inicio_del_contrato', 'fecha_de_fin_del_contrato', 'precio_base', 'valor_total', 'valor_pagado']\n",
      "Escribiendo en capa Bronce (Delta)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/02 01:44:52 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OK - filas: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/02 01:46:19 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "26/02/02 01:46:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "print(\"Leyendo CSV crudo...\")\n",
    "\n",
    "df_raw = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"file:/app/data/SECOP_II_Contratos_Electronicos.csv\")\n",
    ")\n",
    "\n",
    "import re\n",
    "\n",
    "def limpiar_nombre_columna(c: str) -> str:\n",
    "    c = c.strip()\n",
    "    # reemplaza cualquier caracter inválido para Delta por \"_\"\n",
    "    c = re.sub(r\"[ ,;{}\\(\\)\\n\\t=]+\", \"_\", c)\n",
    "    # evita dobles \"__\"\n",
    "    c = re.sub(r\"_+\", \"_\", c)\n",
    "    # evita \"_\" al inicio/fin\n",
    "    c = c.strip(\"_\")\n",
    "    return c.lower()\n",
    "\n",
    "# ver columnas originales\n",
    "print(\"Columnas originales:\")\n",
    "print(df_raw.columns)\n",
    "\n",
    "# renombrar\n",
    "df_bronze = df_raw\n",
    "for c in df_raw.columns:\n",
    "    df_bronze = df_bronze.withColumnRenamed(c, limpiar_nombre_columna(c))\n",
    "\n",
    "print(\"\\nColumnas limpias:\")\n",
    "print(df_bronze.columns)\n",
    "\n",
    "\n",
    "print(\"Escribiendo en capa Bronce (Delta)...\")\n",
    "output_path = \"file:/app/data/lakehouse/bronze/secop\"\n",
    "\n",
    "(df_bronze.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(output_path)\n",
    ")\n",
    "\n",
    "print(\"✅ OK - filas:\", df_bronze.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab84f9d-2ba6-40c0-961e-61a714d92656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
